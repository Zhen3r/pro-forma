---
title: "Predicting rideshare demand in New York City"
author: "Zhenzhao Xu"
output:
  html_document:
    code_folding: hide
    toc: yes
    toc_float: true
editor_options: 
  markdown: 
    wrap: sentence
  chunk_output_type: inline
---

```{r}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	results=FALSE,
	fig.align="center",
	cache=T
)
```

```{r}
library(tidyverse)
library(tidycensus)
library(sf)
library(kableExtra)
library(patchwork)
library(ggplot2)
library(viridis)
library(gridExtra)
library(knitr)
library(lubridate)
library(riem)
library(osmdata)

library(hms)

library(caret)

options(scipen=999)
options(tigris_class = "sf")
options(tigris_use_cache = TRUE)
source("functions.r")

p5 = inferno(5)
p2 = p5[c(4,2)]

v = mapview::mapview

# NAD83(2011) / San Francisco CS13
crs = 7131
```

## 1. Introduction

Motivate the analysis – “What is the use case; why would someone want to replicate your analysis and why would they use this approach?”

Parking fee?
https://data.sfgov.org/Transportation/Meter-Rate-Schedules/fwjv-32uk

parking Citations
https://data.sfgov.org/Transportation/SFMTA-Parking-Citations/ab4h-6ztd

Parking time limits (from ? to ? / up to ? hours)
https://data.sfgov.org/Transportation/Parking-regulations-except-non-metered-color-curb-/hi6h-neyh


https://www.way.com/parking//37.7879938/-122.4074374/Union-Square/

## 2. Data

Meters data
```{r}
meter_trans = read.csv("./data/meter_trans_2021-09.csv")%>%
  mutate(
    interval60 = floor_date(ymd_hms(session_start_dt), unit = "hour"),
    # week = week(interval60),
    # dotw = wday(interval60, label=TRUE),
    parking_time = as.numeric(ymd_hms(session_end_dt)-ymd_hms(session_start_dt)))%>%
  filter(parking_time>0)

meters = st_read("./data/meters.geojson")%>%
  select(post_id, ms_space_num, on_offstreet_type, active_meter_flag, meter_type,
         street_id, street_seg_ctrln_id, longitude, latitude, geometry)%>%
  st_transform(crs)%>%
  mutate(
    street_seg_ctrln_id = street_seg_ctrln_id%>%as.numeric()%>%as.character(),
    ms_space_num = as.numeric(ms_space_num),
    ms_space_num = if_else(ms_space_num==0,1,ms_space_num)
  )
```

Creating Parking Time Panel
```{r eval=FALSE}
parking.time.panel =
  expand.grid(interval60 = unique(weather.Panel$interval60), 
              post_id = unique(meter_trans$post_id))%>%
  mutate(parking_time_in_60m = 0,
         index = row_number())
# meter_trans%>%write_csv("./processed_meter_trans.csv")
```

Processing Parking Time Panel
```{python eval=FALSE}
"""
The function below is used to union the parking time windows of each meter 
and split the time windows into its panel.

[Input]    meterTrans: pd.DataFrame
[Output] parkTimePanel: numpy.array[sympy.Interval[datetime.datetime, datetime.datetime]]
                        (each array element is a parking interval)
"""

for rowNumber in range(meterTrans.shape[0]):
    start_time_rounded = meterTrans.loc[rowNumber, "interval60"]
    start_time = meterTrans.loc[rowNumber, "session_start_dt"].timestamp()
    end_time = meterTrans.loc[rowNumber, "session_end_dt"].timestamp()
    post_id_row = meterTrans.loc[rowNumber, "post_id"]

    while start_time<end_time:
      # get the last minute of this hour
      this_end = min(end_time, start_time + 3600 - start_time%3600)
      this_interval = [start_time, this_end]
      # find the index of this hour and this meter in the meter panel
      index = meterPanel.loc[(post_id_row,start_time_rounded),"rownum"]
      # union the time window
      u = union(array[index], this_interval)
      parkTimePanel[index]=u
      # if parking time > 1hr, continue to the next hour
      start_time_rounded += onehour
      start_time = this_end
```

Loading Processed Parking Time Panel
```{r}
parking.time.panel = read.csv("./data/parking_spot_panel.csv")

parking.time.panel.sum = parking.time.panel%>%
  left_join(meters%>%st_drop_geometry()%>%
      select(street_seg_ctrln_id,post_id), by = "post_id")%>%
  group_by(street_seg_ctrln_id,interval60)%>%
  summarise(parking_time_in_60m = sum(parking_time_in_60m))
parking.time.panel.sum

parking.time.panel.sum%>%
  group_by(interval60)%>%
  summarise(parking_time_in_60m = sum(parking_time_in_60m))%>%
  ggplot()+
    geom_col(aes(ymd_hms(interval60),parking_time_in_60m))

sundays <- 
  mutate(parking.time.panel.sum,
         sunday = ifelse(dotw == "Sun" & hour(interval60) == 1,
                         interval60, 0)) %>%
  filter(sunday != 0) 

parking.time.panel.sum%>%
  filter(interval60>= "2021-09-01 00:00:00" & interval60 <= "2021-09-30 00:00:00")%>%
  group_by(interval60)%>%
  summarise(parking_time_in_60m = sum(parking_time_in_60m))%>%
  ggplot(aes(ymd_hms(interval60),parking_time_in_60m, colour = "red"))+
    geom_line()+
  geom_vline(data = sundays, aes(xintercept = which(levels(crazyfactor) == sunday)))
  
parking.time.panel.sum%>%
  filter(interval60>= "2021-09-01 00:00:00" & interval60 <= "2021-09-16 00:00:00")%>%
  filter(street_seg_ctrln_id == "1363000")%>%
  ggplot(aes(ymd_hms(interval60),parking_time_in_60m, colour = "red"))+
    geom_line()+
   geom_vline(data = sundays, aes(xintercept = sunday))
  
```

Create parking census
```{r}
parking_census <- read.csv("data/On-Street_Parking_Census.csv")%>%
  select(CNN, PRKG_SPLY)
parking_census$CNN <- as.character(parking_census$CNN)

parking_seg_census <- meters%>%
  select(street_seg_ctrln_id, geometry)%>%
  distinct(street_seg_ctrln_id, .keep_all = T)

parking_seg_census <- parking_seg_census%>%
  left_join(parking_census, by = c("street_seg_ctrln_id"="CNN"))%>%
  drop_na(PRKG_SPLY)%>%
  rename(spots_num_census = PRKG_SPLY)

# off_parking_census <- read.csv("data/SFMTA_Managed_Off-street_Parking.csv")%>%
#   select(STREET_SEG_CTRLN_ID, CAPACITY)
```

City amenities data
```{r}
transit <- st_read("data/Muni Stops.geojson")%>%
  st_transform(crs)

park_public <- st_read("data/Recreation and Parks Properties.geojson")%>%
  st_transform(crs)%>%
  select(propertytype, geometry)%>%
  rename(type=propertytype)

park_private <- st_read("data/Privately Owned Public Open Spaces.geojson")%>%
  st_transform(crs)%>%
  select(type, geometry)

cultural_district <- st_read("data/Cultural Districts.geojson")%>%
  st_transform(crs)

st_c = st_coordinates
parking_seg_census <-
  parking_seg_census %>%
  mutate(
    transit.nn =
      nn_function(st_c(parking_seg_census), st_c(transit),1),
    park_public.nn =
      nn_function(st_c(parking_seg_census), st_c(st_centroid(park_public)),1),
    cultural_district.nn =
      nn_function(st_c(parking_seg_census), st_c(st_centroid(cultural_district)),1))
```

OSM data
```{r}
## OSM Data

sf_boundary <- st_read("data/Bay Area Counties.geojson")%>%
  filter(county == "San Francisco")%>%
  st_transform(crs)%>%
  select(geometry)

q0 <- osmdata::opq(bbox = c(-122.3505,37.7025,-122.5171,37.8364)) 

restaurant <- add_osm_feature(opq = q0, key = 'amenity', value = "restaurant") %>%
  osmdata_sf(.)
restaurant.sf <- st_geometry(restaurant$osm_points) %>%
  st_transform(4326) %>%
  st_sf() %>%
  cbind(., restaurant$osm_points$amenity) %>%
  rename(NAME = restaurant.osm_points.amenity)%>%
  st_transform(crs)%>%
  st_intersection(sf_boundary)%>%
  dplyr::select(geometry)%>%
  distinct()

theatre <- add_osm_feature(opq = q0, key = 'amenity', value = "theatre") %>%
  osmdata_sf(.)
theatre.sf <- st_geometry(theatre$osm_points) %>%
  st_transform(4326) %>%
  st_sf() %>%
  cbind(., theatre$osm_points$amenity) %>%
  rename(NAME = theatre.osm_points.amenity)%>%
  st_transform('EPSG:7131')%>%
  st_intersection(sf_boundary)%>%
  dplyr::select(geometry)%>%
  distinct()

cafe <- add_osm_feature(opq = q0, key = 'amenity', value = "cafe") %>%
  osmdata_sf(.)
cafe.sf <- st_geometry(cafe$osm_points) %>%
  st_transform(4326) %>%
  st_sf() %>%
  cbind(., cafe$osm_points$amenity) %>%
  rename(NAME = cafe.osm_points.amenity)%>%
  st_transform('EPSG:7131')%>%
  st_intersection(sf_boundary)%>%
  dplyr::select(geometry)%>%
  distinct()

clothes <- add_osm_feature(opq = q0, key = 'shop', value = "clothes") %>%
  osmdata_sf(.)
clothes.sf <- st_geometry(clothes$osm_points) %>%
  st_transform(4326) %>%
  st_sf() %>%
  cbind(., clothes$osm_points$shop) %>%
  rename(NAME = clothes.osm_points.shop)%>%
  st_transform('EPSG:7131')%>%
  st_intersection(sf_boundary)%>%
  dplyr::select(geometry)%>%
  distinct()

parking_seg_census <-
  parking_seg_census %>%
  mutate(
    restaurant.nn =
      nn_function(st_c(parking_seg_census), st_c(restaurant.sf),3),
    cafe.nn =
      nn_function(st_c(parking_seg_census), st_c(cafe.sf),3),
    theatre.nn =
      nn_function(st_c(parking_seg_census), st_c(theatre.sf),3),
    clothes.nn =
      nn_function(st_c(parking_seg_census), st_c(clothes.sf),3))

park.engineer = parking.time.panel.sum%>%
  left_join(parking_seg_census,by="street_seg_ctrln_id")

weather.Panel$interval60<- as.character(weather.Panel$interval60)

park.engineer <- park.engineer%>%
  left_join(weather.Panel, by = "interval60")
```

adding Time Lag + time dummy
```{r}
park.engineer = park.engineer%>%
  rename(street.id = street_seg_ctrln_id)%>%
  group_by(street.id) %>% 
  mutate(lagHour = dplyr::lag(parking_time_in_60m,1),
         lag2Hours = dplyr::lag(parking_time_in_60m,2),
         lag3Hours = dplyr::lag(parking_time_in_60m,3),
         lag4Hours = dplyr::lag(parking_time_in_60m,4),
         lag12Hours = dplyr::lag(parking_time_in_60m,12),
         lag1day = dplyr::lag(parking_time_in_60m,24),
         lagWeek = dplyr::lag(parking_time_in_60m,24*7),
         week = interval60%>%week,
         weekday = interval60%>%wday(T),
         is.weekend = interval60%>%wday(week_start=1)>=6,
         hour = interval60%>%hour()) %>% 
 ungroup()


```

adding Parking fee
```{r}



```

adding Parking time limits
```{r}



```

nn parking Citations
```{r}



```

Weather
```{r}
weather.SF <- 
  riem_measures(station = "SFO", date_start = "2021-09-01", date_end = "2021-10-01")

weather.Panel <-  
  weather.SF %>%
  mutate_if(is.character, list(~replace(as.character(.), is.na(.), "0"))) %>% 
  replace(is.na(.), 0) %>%
  mutate(interval60 = ymd_h(substr(valid, 1, 13))) %>%
  mutate(week = week(interval60),
         dotw = wday(interval60, label=TRUE)) %>%
  group_by(interval60) %>%
  summarize(Temperature = max(tmpf),
            Percipitation = sum(p01i),
            Wind_Speed = max(sknt)) %>%
  mutate(Temperature = ifelse(Temperature == 0, 42, Temperature))

park.engineer = park.engineer%>%
  mutate(interval60 = ymd_hms(interval60))%>%
  left_join(weather.Panel,by="interval60")
```

## 3. exploratory analysis  using maps and plots

Meter Inconsistency Visualization
```{r}
meter.notfound = setdiff(unique(meter_trans$post_id),unique(meters$post_id))%>%length()
meter.inactive = setdiff(unique(meters$post_id),unique(meter_trans$post_id))%>%length()
meter.intersection = intersect(unique(meters$post_id),unique(meter_trans$post_id))%>%length()
data.frame("meter.inactive"=meter.inactive,
           "meter.notfound"=meter.notfound,
           "meter.intersection"=meter.intersection)%>%
  gather()%>%
ggplot(aes(key,value))+
  geom_col()
```


```{r}
plotData.lag <-
  as.data.frame(park.engineer)%>%
  dplyr::select(starts_with("lag"), parking_time_in_60m) %>%
  gather(Variable, Value, -parking_time_in_60m) %>%
  mutate(Variable = fct_relevel(Variable, "lagHour","lag2Hours","lag3Hours",
                                "lag4Hours","lag12Hours","lag1day"))
correlation.lag <-
  group_by(plotData.lag, Variable) %>%
  summarize(correlation = round(cor(Value, parking_time_in_60m, use = "complete.obs"), 2)) %>%
  kable(caption = "Parking Time in One Hour") %>%
  kable_styling("striped", full_width = F)

correlation.lag
```

```{r}
int.ampeak <- interval(as_hms("07:00:00"),as_hms("10:00:00"))
int.pmpeak <- interval(as_hms("15:00:00"),as_hms("18:00:00"))
int.midday <- interval(as_hms("10:00:00"),as_hms("15:00:00"))
int.night <- interval(as_hms("18:00:00"),as_hms("23:00:00"))
int.overnight <- interval(as_hms("00:00:00"),as_hms("07:00:00"))

parking.time.panel.sum <-parking.time.panel.sum%>%
  mutate(period = case_when(ymd_hms(paste0('1970-01-01',str_sub(interval60, 12)))%within% int.ampeak ~ "AM",
                            ymd_hms(paste0('1970-01-01',str_sub(interval60, 12)))%within% int.pmpeak ~ "PM",
                            ymd_hms(paste0('1970-01-01',str_sub(interval60, 12)))%within% int.midday ~ "Mid_Day",
                            ymd_hms(paste0('1970-01-01',str_sub(interval60, 12)))%within% int.night ~ "Night",
                            ymd_hms(paste0('1970-01-01',str_sub(interval60, 12)))%within% int.overnight ~ "Overnight"))


parking.time.panel.sum %>%
  group_by(interval60, street_seg_ctrln_id, period) %>%
  summarize(mean_time = mean(parking_time_in_60m))%>%
  ggplot()+
  geom_histogram(aes(mean_time), binwidth = 10000)+
  labs(title="Mean Number of Hourly Trips Per Station",
       subtitle="Philadelphia, October 8th - November 11st, 2019",
       x="Mean Parking Time", 
       y="Frequency")+
  facet_wrap(~period)+
  plotTheme()

```

```{r}
park.engineer <-park.engineer%>%
  mutate(period = case_when(ymd_hms(paste0('1970-01-01',str_sub(interval60, 12)))%within% int.ampeak ~ "AM",
                            ymd_hms(paste0('1970-01-01',str_sub(interval60, 12)))%within% int.pmpeak ~ "PM",
                            ymd_hms(paste0('1970-01-01',str_sub(interval60, 12)))%within% int.midday ~ "Mid_Day",
                            ymd_hms(paste0('1970-01-01',str_sub(interval60, 12)))%within% int.night ~ "Night",
                            ymd_hms(paste0('1970-01-01',str_sub(interval60, 12)))%within% int.overnight ~ "Overnight"))

park.engineer %>%
  group_by(interval60, street.id, period) %>%
  summarize(mean_time = mean(parking_time_in_60m))%>%
  ungroup()%>%
  left_join(park.engineer)%>%
  st_as_sf()%>%
ggplot()+
  geom_sf(aes(color=mean_time),
          fill = "transparent", alpha=0.8, size=1)+
  geom_sf(data = sf_neighborhood)+
  facet_grid(~period)
```


```{r}

```


Corr
```{r}

```

### 3.? What is the spatial or space/time process?
```{r}

```




## 4. Describe your modeling approach and show how you arrived at your final model.

```{r}
park.Train = filter(park.engineer, week <= 38) # 35~38
park.Test = filter(park.engineer, week > 38) # 39~40

formula1 = parking_time_in_60m ~ PRKG_SPLY + lagHour + lag2Hours+lag3Hours+lag4Hours+lag12Hours+lag1day+week+weekday+is.weekend+hour
formula2 = parking_time_in_60m ~  PRKG_SPLY + lagHour + lag2Hours+lag3Hours+lag4Hours+lag12Hours+lag1day+week+weekday+is.weekend+hour+transit.nn +park_public.nn+park_public.nn+cultural_district.nn+restaurant.nn+cafe.nn+theatre.nn+clothes.nn+Temperature+Percipitation+Wind_Speed
# formula3 = parking_time_in_60m ~  
# formula4 = parking_time_in_60m ~  

reg1 <- lm(formula1, data=park.Train)
reg2 <- lm(formula2, data=park.Train)
# reg3 <- lm(formula3, data=park.Train)
# reg4 <- lm(formula4,  data=park.Train)
summary(reg1)
summary(reg2)
```

```{r}
model_pred <- function(dat, fit){
   pred <- predict(fit, newdata = dat)}

park.Test.weekNest <- 
  as.data.frame(park.Test) %>%
  nest(-week) 

week_predictions <- 
  park.Test.weekNest %>% 
    mutate(A_Time_FE = map(.x = data, fit = reg1, .f = model_pred),
           B_Time_Space_FE = map(.x = data, fit = reg2, .f = model_pred))

mean.na = function(x) {mean(x,na.rm=T)}
 sd.na = function(x) {sd(x,na.rm=T)}

week_predictions <- week_predictions %>%  
    gather(Regression, Prediction, -data, -week) %>% 
    mutate(Observed = map(data, pull, Trip_Count),
           Absolute_Error = map2(Observed, Prediction,  ~ abs(.x - .y)),
           MAE = map_dbl(Absolute_Error, mean.na),
           sd_AE = map_dbl(Absolute_Error, sd.na))

week_predictions %>%
  dplyr::select(week, Regression, MAE) %>%
  gather(Variable, MAE, -Regression, -week) %>%
  ggplot(aes(week%>%as.factor(), MAE)) + 
    geom_bar(aes(fill = Regression), position = "dodge", stat="identity") +
    scale_fill_manual(values = p5_2[1:4]) +
    labs(title = "Mean Absolute Errors by model specification and week",x="week") +
  plotTheme()

```


## 5. Validate your model with cross-validation and describe how your predictions are useful (accuracy vs. generalizability).
```{r}


# cross-validation
fitControl <- caret::trainControl(method = "cv", number = 5)
set.seed(1)
reg.cv = caret::train(formula2, data = park.engineer, 
     method = "lm", trControl = fitControl, na.action = na.pass)
summary(reg.cv)
```


```{r}
week_predictions <- 
  ride.Test.weekNest %>% 
    mutate(A_Time_FE = map(.x = data, fit = reg1, .f = model_pred),
           B_Space_FE = map(.x = data, fit = reg2, .f = model_pred),
           C_Space_Time_FE = map(.x = data, fit = reg3, .f = model_pred),
           D_Space_Time_Lags = map(.x = data, fit = reg4, .f = model_pred))

mean.na = function(x) {mean(x,na.rm=T)}
 sd.na = function(x) {sd(x,na.rm=T)}

week_predictions <- week_predictions %>%  
    gather(Regression, Prediction, -data, -week) %>% 
    mutate(Observed = map(data, pull, Trip_Count),
           Absolute_Error = map2(Observed, Prediction,  ~ abs(.x - .y)),
           MAE = map_dbl(Absolute_Error, mean.na),
           sd_AE = map_dbl(Absolute_Error, sd.na))

week_predictions %>%
  dplyr::select(week, Regression, MAE) %>%
  gather(Variable, MAE, -Regression, -week) %>%
  ggplot(aes(week%>%as.factor(), MAE)) + 
    geom_bar(aes(fill = Regression), position = "dodge", stat="identity") +
    scale_fill_manual(values = p5_2[1:4]) +
    labs(title = "Mean Absolute Errors by model specification and week",x="week") +
  plotTheme()
```

```{r}




```


## 6. Provide additional maps and data visualizations to show that your model is useful.
```{r}




```





## 7. Discussion

### 7.1 Talk about how your analysis meets the use case you set out to address.
### 7.2 What could you do to make the analysis better?